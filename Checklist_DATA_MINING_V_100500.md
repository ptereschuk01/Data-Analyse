# Checklist data mining

0. Context and Constraints (Business Context)

0.1 Description of the Business Context and Domain  
Industry: [specify the industry, e.g., telecommunications, retail, manufacturing].  
Brief context: business problem/task, current situation, why the analysis is important.  
Example: â€œThe goal of the project is to reduce customer churn in the postpaid contract segmentâ€.  

0.2 Stakeholders and End Users of the Results  
Main stakeholders: department heads, management, marketing, sales/customer service.  
End users: teams that make decisions based on the analysis results (CRM, dashboard, BI tools).  

0.3 Business Metrics (KPI)  
KPIs by which the impact of the analysis is evaluated:  
Main metrics: [e.g., Churn Rate, Retention Rate, ARPU]  
Additional metrics: campaign ROI, average revenue per customer, customer satisfaction  

0.4 Constraints and Assumptions  
Data: quality, completeness, volume, update delays  
Time and resources: project deadlines, available team, computing capacity  
Model: requirements for interpretability, reproducibility, explainability  
Legal and ethical aspects: GDPR compliance, protection of personal data, avoidance of bias  

1. Problem Definition (Business Understanding)  
1.1 Objectives of the Analysis  

1.1.1 Business Problem  
What needs to be solved: [e.g., identification of customers with high churn risk]  

1.1.2 Success Criteria  
How the result is measured: reduction of Churn Rate by X% within Y months, increase in Retention Rate, increase in ROI of retention campaigns.  

1.2 Key Analytical Questions  

1.2.1 Main Questions  
Which factors influence [churn, revenue, productivity]?  
Is it possible to predict the behavior of customers or employees?  

1.2.2 Hypotheses to Be Tested (priority / if possible)  
Customers with frequent support requests churn more often  
Behavioral changes (usage volume, purchases) signal risk  
Additional project-specific business hypotheses  

1.3 Decisions and Management Actions Based on the Analysis  

1.3.1 Possible Actions  
Targeted retention campaigns  
Personalized offers  
Optimization of service or sales processes  

1.3.2 Use of Results  
The results are integrated into business processes via CRM, BI tools, or dashboards  
Who uses them: marketing, management, customer support  
The results are regularly monitored and evaluated based on KPIs  

2. Data Collection (Data Understanding)  

2.1 Data Sources  

2.1.1 Internal Databases  
SQL (Oracle, PostgreSQL, MySQL)  
NoSQL (MongoDB, Cassandra)  

2.1.2 File-Based Sources  
CSV, Excel, Parquet  

2.1.3 API  
Internal services, external REST/SOAP APIs  

2.1.4 Web Scraping  
Websites, portals, open sources  

2.1.5 External Sources  
Open Data, Kaggle, government data, partner data  

ðŸ”¹ Recommendation: Always document the retrieval date and the source version.  

2.2 Data Acquisition and Documentation  
Loading data from all defined sources  

Documentation of the data structure:  
table and field names, data types  
sources and retrieval date  
example values  

Documentation of all transformations (ETL processes)  

2.3 Relevance and Sufficiency Check  
Do the data correspond to the analysis objectives?  
Is the volume and depth sufficient for modeling or statistical analysis?  
Are there additional features that can be collected or generated (feature engineering)?  

2.4 Data Quality Check  
Completeness: missing values, empty fields, NA  
Plausibility of value ranges: dates, numerical indicators, categories  
Consistency between sources: matching keys, formats, duplicates  
Timeliness: data freshness, update date  
Additionally: identification of anomalies, outliers, logical integrity  

ðŸ”¹ Recommendation: Document all checks in a table or a Data Quality Report to present them to stakeholders.  

3. Data Preparation  

3.1 Data Loading and Initial Review  

3.1.0 Environment Setup  
Python, Pandas, NumPy, Scikit-learn and other libraries.  
Connection to storage locations: Google Drive, local files, databases.  

3.1.1 Importing Libraries and Loading Data  
pd.read_csv, pd.read_excel, SQL queries or API.  
Verification of successful import, number of rows/columns.  

3.1.2 Initial Data Overview (Exploratory Check)  
Table dimensions, data types, sample records.  
Basic statistics: mean, median, std, min/max, quantiles.  
Immediately visible issues: missing values, duplicates, outliers.  

3.2 Data Cleaning / Preprocessing  

3.2.1 Handling Missing Values  
Removal of rows or columns with critical missing values.  
Imputation: mean, median, mode, â€œUnknownâ€, 0, etc.  
Documentation of the chosen strategy.  

3.2.2 Removal of Duplicates and Non-Informative Features  

3.2.2.1 Technical Identifiers  
ID, UUID, ticket numbers, indices  

3.2.2.2 Features with Zero or Near-Zero Variance  
Identical values in all rows  

3.2.2.3 Redundant Features  
Derived from each other (e.g., date and year)  

3.2.2.4 Columns with Identical Values  
E.g., column â€œGenderâ€ if all entries are â€œMaleâ€  

3.2.3 Treatment of Outliers  
Cause analysis: error, rare case, real extreme value  
Removal or transformation (logarithm, winsorization)  

3.2.4 Exclusion of Clearly Non-Informative Data  
Features without influence on the target variable: random codes, indices, technical fields  

3.3 Data Transformation (Feature Engineering / Transformation)  

3.3.0 Definition of the Target Variable  
What is predicted and at what point in time.  

3.3.1 Conversion of Data Types  
Dates â†’ datetime  
Categories â†’ category  
Numeric â†’ float/int if necessary  

3.3.2 Separation of Features by Type  
Quantitative: age, price, income, number of purchases  
Qualitative: color, gender, city, car brand, payment method  
Binary: 1/0, Yes/No  

3.3.3 Quantitative Features  
Keep as is or standardize (StandardScaler) / normalize (MinMaxScaler)  

3.3.4 Encoding of Categorical Features  
Ordinal Encoding: if there is a natural order (â€œbadâ€, â€œaverageâ€, â€œgoodâ€)  
One-Hot Encoding: if order is not relevant  
Label Encoding: if there are few categories and numerical interpretation is acceptable  

3.3.5 Creation of New Features (Feature Engineering)  
Aggregations (mean, sum, max/min)  
Ratios  
Time features (year, month, day of week, season)  
Domain-specific features (e.g., â€œnumber of purchases in the last 30 daysâ€)  

3.3.6 Data Leakage Check  
Exclusion of features containing future information or directly dependent on the target variable  

ðŸ”¹ Additional Recommendations:  
1. Document all steps â€” especially for missing values, duplicates, and encoding.  
2. Visual data control â€” hist, boxplot, value_counts for categories.  
3. Logical plausibility check â€” e.g., age > 0, purchase date â‰¤ today.  
4. Save intermediate versions after each preprocessing step.  

8. Exploratory Data Analysis (Exploratory Data Analysis, EDA)  

4.1 Descriptive Statistics  
Calculation of key metrics for all quantitative features:  
Mean (mean)  
Median (median)  
Standard deviation (std)  
Minimum/maximum (min/max)  
Quartiles/percentiles (25%, 50%, 75%)  

4.2 Distribution Analysis  
Checking normality and feature distributions  
Determination of skewness and kurtosis  
Identification of anomalies and outliers  

4.3 Data Visualization  
Histograms â€“ for distributions of quantitative features  
Boxplot â€“ for identification of outliers  
Scatter plots â€“ for analyzing the relationship between two quantitative features  
Heatmap â€“ for visualization of correlations  
Bar chart â€“ for categorical features  

4.4 Correlation Analysis  
Checking linear relationships between quantitative features  
For categorical features â€“ chi2 or CramÃ©r's V  
Identification of multicollinearity for modeling   

4.5 Segmentation and Hypothesis Testing  
Dividing the data into groups by key features (e.g., Pclass, gender, age groups)  
Hypothesis testing using statistical methods:  
t-test, ANOVA â€“ for quantitative features  
chi2 â€“ for categorical features  

ðŸ”¹ Additionally:  
1. Document all charts and statistical results in the report/notebook  
2. Record conclusions in Markdown: which features are important, which outliers should be removed, which hypotheses are confirmed  
3. EDA forms the basis for feature engineering and model selection  
